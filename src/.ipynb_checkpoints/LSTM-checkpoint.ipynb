{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_arr = [0]*54\n",
    "for i in range(0,54):\n",
    "    col_arr[i] = i\n",
    "\n",
    "path1 ='data/Day1/Post/DataFeatures1.csv'\n",
    "path2 ='data/Day2/Post1/DataFeatures2.csv' \n",
    "path3 ='data/Day2/Post2/DataFeatures.csv' \n",
    "path4 ='data/Day3/Post/DataFeatures4.csv'\n",
    "path5 ='data/Day4/Post/DataFeatures.csv'\n",
    "\n",
    "df1=(pd.read_csv(path1 ,header = None, usecols = col_arr))\n",
    "df2=(pd.read_csv(path2 ,header = None, usecols = col_arr))\n",
    "df3=(pd.read_csv(path3 ,header = None, usecols = col_arr))\n",
    "df4=(pd.read_csv(path4 ,header = None, usecols = col_arr))\n",
    "df5=(pd.read_csv(path5 ,header = None, usecols = col_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = np.array(df1)\n",
    "df2 = np.array(df2)\n",
    "df3 = np.array(df3)\n",
    "df4 = np.array(df4)\n",
    "df5 = np.array(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.concatenate((df1, df2, df3, df4, df5))\n",
    "df = pd.DataFrame(df)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = [i for i in range(1, 53)]\n",
    "Y = df[53]\n",
    "X = df[f]\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y = np.expand_dims(Y, axis=1)\n",
    "\n",
    "X = X[:-520]\n",
    "Y = Y[:-520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_split = 0.4\n",
    "len_train = int( X.shape[0] * (1 - cv_split) )\n",
    "len_CV = int( X.shape[0] * cv_split )\n",
    "\n",
    "X_train = X[0 : len_train]\n",
    "Y_train = Y[0 : len_train]\n",
    "X_CV = X[len_train : ]\n",
    "Y_CV = Y[len_train : ]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_CV.shape)\n",
    "print(Y_CV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(Y_train[0:3000])\n",
    "#plt.xlabel('Timestamp')\n",
    "#plt.ylabel('Breaths/min')\n",
    "#plt.title('Respiratory Rate (Zoomed)')\n",
    "#plt.savefig('assets/RR_zoomed.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_train)\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Breaths/min')\n",
    "plt.title('Respiratory Rate (Total)')\n",
    "#plt.savefig('assets/RR_total.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = np.gradient(Y_train[:,0])\n",
    "abs_grad = np.absolute(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(abs_grad)\n",
    "plt.title('Derivative')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Respiratory Rate')\n",
    "plt.savefig('assets/grad.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 5 \n",
    "for i in range(abs_grad.shape[0]):\n",
    "    if(abs_grad[i] < thresh):\n",
    "        abs_grad[i] = 0\n",
    "\n",
    "plt.axhline(y=thresh, color='r', linestyle='-')\n",
    "plt.plot(abs_grad)\n",
    "plt.title('Derivative')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Respiratory Rate')\n",
    "#plt.savefig('assets/grad_thresh.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(abs_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train[  0   : 24270]\n",
    "Y_train1 = Y_train[  0   : 24270]\n",
    "\n",
    "X_train2 = X_train[25500 : 47600]\n",
    "Y_train2 = Y_train[25500 : 47600]\n",
    "\n",
    "X_train3 = X_train[50000 : 91300]\n",
    "Y_train3 = Y_train[50000 : 91300]\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(Y_train1)\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(Y_train2)\n",
    "plt.ylabel('Respiratory Rate')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(Y_train3)\n",
    "plt.xlabel('Timestamps')\n",
    "\n",
    "meanRate = (np.sum(Y_train1) + np.sum(Y_train2) + np.sum(Y_train3))/(Y_train1.shape[0]+Y_train2.shape[0]+Y_train3.shape[0])\n",
    "print(meanRate)\n",
    "\n",
    "plt.savefig('assets/sets.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pca_check = PCA()\n",
    "pca_check.fit(X_train1)\n",
    "pca_check.fit(X_train2)\n",
    "pca_check.fit(X_train3)\n",
    "\n",
    "print(pca_check.explained_variance_ratio_)\n",
    "plt.stem(pca_check.explained_variance_ratio_)\n",
    "#print(\"\\n\")\n",
    "#print(pca_check.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 9\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "pca.fit(X_train1)\n",
    "pca.fit(X_train2)\n",
    "pca.fit(X_train3)\n",
    "\n",
    "X1_pca = pca.transform(X_train1)\n",
    "X2_pca = pca.transform(X_train2)\n",
    "X3_pca = pca.transform(X_train3)\n",
    "\n",
    "print(X1_pca.shape)\n",
    "print(X2_pca.shape)\n",
    "print(X3_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing all the features with Zero Mean and Unit Variance\n",
    "scalerX = StandardScaler()\n",
    "scalerY = StandardScaler()\n",
    "\n",
    "scalerX.fit(X1_pca)\n",
    "scalerX.fit(X2_pca)\n",
    "scalerX.fit(X3_pca)\n",
    "\n",
    "scalerY.fit(Y_train1)\n",
    "scalerY.fit(Y_train2)\n",
    "scalerY.fit(Y_train3)\n",
    "\n",
    "\n",
    "X_train1 = scalerX.transform(X1_pca)\n",
    "X_train2 = scalerX.transform(X2_pca)\n",
    "X_train3 = scalerX.transform(X3_pca)\n",
    "\n",
    "Y_train1 = scalerY.transform(Y_train1)\n",
    "Y_train2 = scalerY.transform(Y_train2)\n",
    "Y_train3 = scalerY.transform(Y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same transformations in the Test/Cross-Validation data\n",
    "X_CV = pca.transform(X_CV)\n",
    "X_CV = scalerX.transform(X_CV)\n",
    "\n",
    "Y_CV = scalerY.transform(Y_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeqData(X, Y, seqlength):\n",
    "    x, y = [], []\n",
    "    for i in range(X.shape[0]-seqlength):\n",
    "        x.append(X[i : i+seqlength])\n",
    "        y.append(Y[i+seqlength])\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 64\n",
    "XT1, YT1 = getSeqData(X_train1, Y_train1, sequence_length)\n",
    "XT1, _, YT1, _ = train_test_split(XT1, YT1, test_size=0, random_state=2)\n",
    "print(XT1.shape)\n",
    "print(YT1.shape)\n",
    "\n",
    "XT2, YT2 = getSeqData(X_train2, Y_train2, sequence_length)\n",
    "XT2, _, YT2, _ = train_test_split(XT2, YT2, test_size=0, random_state=2)\n",
    "print(XT2.shape)\n",
    "print(YT2.shape)\n",
    "\n",
    "XT3, YT3 = getSeqData(X_train3, Y_train3, sequence_length)\n",
    "XT3, _, YT3, _ = train_test_split(XT3, YT3, test_size=0, random_state=2)\n",
    "print(XT3.shape)\n",
    "print(YT3.shape)\n",
    "\n",
    "X_CV, Y_CV = getSeqData(X_CV, Y_CV, sequence_length)\n",
    "print(X_CV.shape)\n",
    "print(Y_CV.shape)\n",
    "\n",
    "len_train1 = X_train1.shape[0]\n",
    "len_train2 = X_train2.shape[0]\n",
    "len_train3 = X_train3.shape[0]\n",
    "len_CV = X_CV.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reference : \n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/dynamic_rnn.ipynb\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "input_dimension = n_components\n",
    "num_hidden1 = 128\n",
    "num_hidden2 = 128\n",
    "\n",
    "drop_prob = 0.3\n",
    "\n",
    "train_loss_arr = []\n",
    "cv_loss_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batches1 = int(len_train1/batch_size)\n",
    "total_batches2 = int(len_train2/batch_size)\n",
    "total_batches3 = int(len_train3/batch_size)\n",
    "total_batches_CV = int(len_CV/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, sequence_length, input_dimension])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Define weights\n",
    "weights = {'lstm2fc1': tf.Variable(tf.random_normal([num_hidden2, 64])), \n",
    "           'fc1_fc2': tf.Variable(tf.random_normal([64, 1]))}\n",
    "\n",
    "biases = {'lstm2fc1': tf.Variable(tf.random_normal([64])), \n",
    "          'fc1_fc2': tf.Variable(tf.random_normal([1]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNCell(x, sequence_length, num_hidden):\n",
    "    x_unstacked = tf.unstack(x, sequence_length, 1)\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_hidden)\n",
    "    outputs, states = tf.nn.static_rnn(lstm_cell, x_unstacked, dtype=tf.float32)\n",
    "    return outputs\n",
    "\n",
    "# Reference : https://www.oreilly.com/ideas/introduction-to-lstms-with-tensorflow\n",
    "def MultiRNNCell(x, sequence_length, batch_size, num_hidden1, num_hidden2):\n",
    "    \n",
    "    x_unstacked = tf.unstack(x, sequence_length, 1)\n",
    "    \n",
    "    lstm_cell1 = tf.nn.rnn_cell.BasicLSTMCell(num_hidden1, state_is_tuple=True)\n",
    "    lstm_cell2 = tf.nn.rnn_cell.BasicLSTMCell(num_hidden2, state_is_tuple=True)\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell1, lstm_cell2], state_is_tuple=True)\n",
    "    \n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    lstm_outputs, final_state = tf.nn.static_rnn(cell, x_unstacked, initial_state=init_state)\n",
    "    \n",
    "    return lstm_outputs\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = MultiRNNCell(x, sequence_length, batch_size, num_hidden1, num_hidden2)\n",
    "\n",
    "drop1 = tf.layers.dropout(outputs[-1], rate=drop_prob)\n",
    "layer1 = tf.matmul(drop1, weights['lstm2fc1']) + biases['lstm2fc1']\n",
    "act1 = tf.nn.relu(layer1)\n",
    "\n",
    "pred = tf.matmul(act1, weights['fc1_fc2']) + biases['fc1_fc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost =  tf.losses.mean_squared_error(pred, y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    if(os.path.isfile('model/model.meta')):\n",
    "        saver_restore = tf.train.import_meta_graph('model/model.meta')\n",
    "        saver_restore.restore(sess, 'model/model')\n",
    "        print('Restored Model Successfully')\n",
    "        \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        train_loss = 0\n",
    "        for batch in range(total_batches1):\n",
    "            batch_x = XT1[batch*batch_size : (batch+1)*batch_size]\n",
    "            batch_y = YT1[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(optimizer, feed_dict={x : batch_x, y : batch_y})\n",
    "            train_loss += sess.run(cost, feed_dict={x : batch_x, y : batch_y})\n",
    "        \n",
    "        for batch in range(total_batches2):\n",
    "            batch_x = XT2[batch*batch_size : (batch+1)*batch_size]\n",
    "            batch_y = YT2[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(optimizer, feed_dict={x : batch_x, y : batch_y})\n",
    "            train_loss += sess.run(cost, feed_dict={x : batch_x, y : batch_y})\n",
    "        \n",
    "        for batch in range(total_batches3):\n",
    "            batch_x = XT3[batch*batch_size : (batch+1)*batch_size]\n",
    "            batch_y = YT3[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(optimizer, feed_dict={x : batch_x, y : batch_y})\n",
    "            train_loss += sess.run(cost, feed_dict={x : batch_x, y : batch_y})\n",
    "        \n",
    "        train_loss_arr.append(train_loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        cv_loss = 0\n",
    "        for batch in range(total_batches_CV):\n",
    "            batch_x = X_CV[batch*batch_size : (batch+1)*batch_size]\n",
    "            batch_y = Y_CV[batch*batch_size : (batch+1)*batch_size]\n",
    "            cv_loss += sess.run(cost, feed_dict={x : batch_x, y : batch_y})\n",
    "        \n",
    "        cv_loss_arr.append(cv_loss)\n",
    "        \n",
    "        save_path = saver.save(sess, \"model/model\")\n",
    "        \n",
    "        print('Epoch :', epoch+1, '   Train Loss :', train_loss, '   CV Loss :', cv_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "tr, = plt.plot(train_loss_arr, label='Train Loss', marker='o')\n",
    "cv, = plt.plot(cv_loss_arr, label='Validation Loss', marker='o')\n",
    "plt.legend([tr, cv], ['Train Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Mean Squared Error')\n",
    "#plt.savefig('assets/loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver_restore = tf.train.import_meta_graph('model/model.meta')\n",
    "    saver_restore.restore(sess, 'model/model')\n",
    "    \n",
    "    for batch in range(total_batches_CV):\n",
    "            batch_x = X_CV[batch*batch_size : (batch+1)*batch_size]\n",
    "            batch_y = Y_CV[batch*batch_size : (batch+1)*batch_size]\n",
    "            out = sess.run(pred, feed_dict={x : batch_x, y : batch_y})\n",
    "            Y_pred.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.array(Y_pred)\n",
    "prediction = r.ravel()\n",
    "\n",
    "pred_len = prediction.shape[0]\n",
    "original_len = Y_CV.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = scalerY.inverse_transform(Y_CV)\n",
    "prediction = scalerY.inverse_transform(prediction)\n",
    "\n",
    "pad = [meanRate] * (original_len - pred_len)\n",
    "pad = np.array(pad)\n",
    "prediction = np.append(pad, prediction)\n",
    "\n",
    "error = np.sqrt(mean_squared_error(original, prediction))\n",
    "print('RMSE =', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b9681bb3a796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrue_reading\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted_reading\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_reading\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_reading\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Breaths/min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "true_reading, = plt.plot(original, label='True')\n",
    "predicted_reading, = plt.plot(prediction, label='Predicted')\n",
    "plt.legend([true_reading, predicted_reading], ['True', 'Predicted'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Breaths/min')\n",
    "plt.title('Respiratory Rate')\n",
    "#plt.savefig('assets/Prediction.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
